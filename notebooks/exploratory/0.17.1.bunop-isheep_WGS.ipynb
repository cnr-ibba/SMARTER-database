{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193de060-b877-41be-a31b-d7fdf64441e3",
   "metadata": {},
   "source": [
    "# ISHEEP WGS DATASET\n",
    "\n",
    "This is an attempt to collect SNPs from [isheep WGS dataset](https://ngdc.cncb.ac.cn/isheep/download) and track them in to smarter database. There absolutely no info regarding *probeset IDs*, so the only way to collect SNPs from this dataset relies on position on *OAR4*. Data is divided by chromosomes and where downloaded from [isheep WGS ftp folder](ftp://download.big.ac.cn/isheep/SNP).\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "Files were compressed using the standard `gzip` utility. Unpack all files and compress them using `bgzip`, then index with `tabix`:\n",
    "\n",
    "```bash\n",
    "for compressed in $(ls *.vcf.gz); do echo \"Processing \" $compressed; vcf=\"${compressed%.*}\"; bgzip -d --stdout $compressed | bgzip -@24 --compress-level 9 --stdout > $vcf.bgzip ; done\n",
    "for compressed in $(ls *.bgzip); do echo \"Processing \" $compressed; bgzip --test $compressed ; done\n",
    "for compressed in $(ls *.bgzip); do echo \"Processing \" $compressed; vcf=\"${compressed%.*}.gz\" ; mv $compressed $vcf ; tabix $vcf ; done\n",
    "```\n",
    "\n",
    "Now I can try to extract my probes relying on positions and `tabix`. The point is that I could have more probe on the same position, so I can't assign a unique `VariantSheep.name` to a certain SNP; Moreover positions need to be checked against `OAR4` before querying VCF. For the moment, I will try to extract the information I need relying on the data I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4ee7c5-0390-422e-853e-7dcd8cd69e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.features.smarterdb import global_connection, VariantSheep\n",
    "from src.features.utils import get_interim_dir\n",
    "from src.data.common import WORKING_ASSEMBLIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e30661-e393-4b83-ae6b-e34cbcada3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = global_connection()\n",
    "OAR4 = WORKING_ASSEMBLIES[\"OAR4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84d558-e454-4473-ad35-1b30aa741f07",
   "metadata": {},
   "source": [
    "## Creating the regions file\n",
    "\n",
    "As described by the `tabix` documentation, I can collect samples by region. Chromosome and position in a *tab separated* file is enough. I have a VCF for each chromosome, so I need to collect data by chromosomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e1d064-0a62-48f8-8583-9a2a12eb992e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec6f43a451a4dd6907cd95952b59956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chromosomes = [str(chrom) for chrom in range(1, 27)] + [\"X\"]\n",
    "\n",
    "for chrom in tqdm(chromosomes):\n",
    "    condition = OAR4._asdict()\n",
    "    condition['chrom'] = chrom\n",
    "    variants = VariantSheep.objects.filter(\n",
    "        locations__match=condition\n",
    "    ).fields(\n",
    "        elemMatch__locations=OAR4._asdict(),\n",
    "        name=1,\n",
    "        rs_id=1\n",
    "    )\n",
    "    \n",
    "    with open(get_interim_dir() / f\"chr{chrom}_regions.tsv\", \"w\") as handle:\n",
    "        writer = csv.writer(handle, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "        for variant in variants:\n",
    "            location = variant.locations[0]\n",
    "            writer.writerow([location.chrom, location.position])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce87705-f900-4d1f-a8dc-ef4c127ffaa9",
   "metadata": {},
   "source": [
    "## Extract variants from VCF\n",
    "\n",
    "It's time to extract the sheep smarter variants from the VCF files. Then merge all files into one VCF:\n",
    "\n",
    "```bash\n",
    "for i in $(seq 1 26); do tabix -h -R chr$i\\_regions.tsv output_chr$i.snp.filtered.vcf.gz | bgzip --compress-level 9 --stdout > smarter_chr$i\\_regions.vcf.gz; tabix smarter_chr$i\\_regions.vcf.gz ; done\n",
    "tabix -h -R chrX_regions.tsv output_chrX.snp.filtered.vcf.gz | bgzip --compress-level 9 --stdout > smarter_chrX_regions.vcf.gz\n",
    "tabix smarter_chrX_regions.vcf.gz\n",
    "vcf-concat smarter_chr*.vcf.gz | bgzip -@24 --stdout > WGS-all/WGS-all.smarter.vcf.gz\n",
    "tabix WGS-all/WGS-all.smarter.vcf.gz\n",
    "plink --allow-extra-chr --vcf WGS-all/WGS-all.smarter.vcf.gz --make-bed --double-id --out WGS-all/WGS-all.smarter\n",
    "```\n",
    "\n",
    "Please note that in the final plink file `WGS-all.smarter.bim` SNPs have no name: this need to be fixed with the `VariantSheep.name` field in order to be merged with the SMARTER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c507b9-b22c-48b6-a8f4-e4f0a4545e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
